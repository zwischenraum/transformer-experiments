run:
  project: transformer
  name: train
  artifact:
    filename: checkpoint.pt
    name: model-checkpoint
  id: null
  resume_artifact: null

training:
  total_steps: 50000
  batch_size: 100
  learning_rate: 0.0003
  seq_len: 256
  checkpoint_interval_steps: 5000
  warmup_steps: 2000

model:
  d_model: 768
  n_heads: 12
  n_layers: 12
  d_ff: 3072
  dropout: 0.1

dataset:
  path: HuggingFaceFW/fineweb-edu
  name: sample-10BT
  split: train
  streaming: true
  tokenizer: openai-community/gpt2

  tokenizer_batch_size: 50
  shuffle:
    seed: 42
    buffer_size: 10000
